{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artificial-fence",
   "metadata": {},
   "source": [
    "Script to run baseline:\n",
    "\n",
    "```\n",
    "\n",
    "python train.py link_prediction with \\                                     \n",
    "dataset='FB15k-237' \\\n",
    "inductive=True \\\n",
    "model='bert-dkrl' \\\n",
    "rel_model='transe' \\\n",
    "loss_fn='margin' \\\n",
    "regularizer=1e-2 \\\n",
    "max_len=32 \\\n",
    "num_negatives=64 \\\n",
    "lr=1e-4 \\\n",
    "use_scheduler=False \\\n",
    "batch_size=64 \\\n",
    "emb_batch_size=512 \\\n",
    "eval_batch_size=128 \\\n",
    "max_epochs=5 \\\n",
    "checkpoint=None \\\n",
    "use_cached_text=False\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tough-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sacred.run import Run\n",
    "from logging import Logger\n",
    "from sacred import Experiment\n",
    "from sacred.observers import MongoObserver\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "#from data import CATEGORY_IDS\n",
    "#from data import GraphDataset, TextGraphDataset, GloVeTokenizer\n",
    "#import models\n",
    "#import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chubby-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "radio-champion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suparnaghanvatkar/anaconda3/envs/blp/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings, TransformerWordEmbeddings, WordEmbeddings\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stopped-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transe_score(heads, tails, rels):\n",
    "    return -torch.norm(heads + rels - tails, dim=-1, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlled-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(pos_scores, neg_scores):\n",
    "    loss = 1 - pos_scores + neg_scores\n",
    "    loss[loss < 0] = 0\n",
    "    return loss.mean()\n",
    "\n",
    "def nll_loss(pos_scores, neg_scores):\n",
    "    return (F.softplus(-pos_scores).mean() + F.softplus(neg_scores).mean()) / 2\n",
    "\n",
    "\n",
    "def l2_regularization(heads, tails, rels):\n",
    "    reg_loss = 0.0\n",
    "    for tensor in (heads, tails, rels):\n",
    "        reg_loss += torch.mean(tensor ** 2)\n",
    "\n",
    "    return reg_loss / 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suburban-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPrediction(nn.Module):\n",
    "    \"\"\"A general link prediction model with a lookup table for relation\n",
    "    embeddings.\"\"\"\n",
    "    def __init__(self, dim, rel_model, loss_fn, num_relations, regularizer):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.normalize_embs = False\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "        if rel_model == 'transe':\n",
    "            self.score_fn = transe_score\n",
    "            self.normalize_embs = True\n",
    "        else:\n",
    "            raise ValueError(f'Unknown relational model {rel_model}.')\n",
    "\n",
    "        self.rel_emb = nn.Embedding(num_relations, self.dim)\n",
    "        nn.init.xavier_uniform_(self.rel_emb.weight.data)\n",
    "\n",
    "        if loss_fn == 'margin':\n",
    "            self.loss_fn = margin_loss\n",
    "        elif loss_fn == 'nll':\n",
    "            self.loss_fn = nll_loss\n",
    "        else:\n",
    "            raise ValueError(f'Unkown loss function {loss_fn}')\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        ent_emb = self._encode_entity(*args, **kwargs)\n",
    "        if self.normalize_embs:\n",
    "            ent_emb = F.normalize(ent_emb, dim=-1)\n",
    "\n",
    "        return ent_emb\n",
    "\n",
    "    def _encode_entity(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_loss(self, ent_embs, rels, neg_idx):\n",
    "        batch_size = ent_embs.shape[0]\n",
    "\n",
    "        # Scores for positive samples\n",
    "        rels = self.rel_emb(rels)\n",
    "        heads, tails = torch.chunk(ent_embs, chunks=2, dim=1)\n",
    "        pos_scores = self.score_fn(heads, tails, rels)\n",
    "\n",
    "        if self.regularizer > 0:\n",
    "            reg_loss = self.regularizer * l2_regularization(heads, tails, rels)\n",
    "        else:\n",
    "            reg_loss = 0\n",
    "\n",
    "        # Scores for negative samples\n",
    "        neg_embs = ent_embs.view(batch_size * 2, -1)[neg_idx]\n",
    "        heads, tails = torch.chunk(neg_embs, chunks=2, dim=2)\n",
    "        neg_scores = self.score_fn(heads.squeeze(), tails.squeeze(), rels)\n",
    "\n",
    "        model_loss = self.loss_fn(pos_scores, neg_scores)\n",
    "        return model_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "together-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InductiveLinkPrediction(LinkPrediction):\n",
    "    \"\"\"Description-based Link Prediction (DLP).\"\"\"\n",
    "    def _encode_entity(self, text_tok, text_mask):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, text, rels=None, neg_idx=None):\n",
    "\n",
    "        # Encode text into an entity representation from its description\n",
    "        ent_embs = self.encode(text)\n",
    "\n",
    "        if rels is None and neg_idx is None:\n",
    "            # Forward is being used to compute entity embeddings only\n",
    "            out = ent_embs\n",
    "        else:\n",
    "            # Forward is being used to compute link prediction loss\n",
    "            ent_embs = ent_embs.view(batch_size, 2, -1)\n",
    "            out = self.compute_loss(ent_embs, rels, neg_idx)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prepared-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingsLP(InductiveLinkPrediction):\n",
    "    \"\"\"Description encoder with pretrained embeddings, obtained from BERT or a\n",
    "    specified tensor file.\n",
    "    \"\"\"\n",
    "    def __init__(self, rel_model, loss_fn, num_relations, regularizer,\n",
    "                 dim=None, encoder_name=None, embeddings=None):\n",
    "        if not encoder_name and not embeddings:\n",
    "            raise ValueError('Must provided one of encoder_name or embeddings')\n",
    "\n",
    "        if encoder_name is not None:\n",
    "            encoder = TransformerWordEmbeddings(encoder_name)\n",
    "        else:\n",
    "            #then it is GLOVE in this case\n",
    "            encoder = WordEmbeddings('glove')\n",
    "\n",
    "        super().__init__(dim, rel_model, loss_fn, num_relations, regularizer)\n",
    "\n",
    "        self.embeddings = encoder\n",
    "\n",
    "    def _encode_entity(self, text_tok, text_mask):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stone-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKRL(WordEmbeddingsLP):\n",
    "    \"\"\"Description-Embodied Knowledge Representation Learning (DKRL) with CNN\n",
    "    encoder, after\n",
    "    Zuo, Yukun, et al. \"Representation learning of knowledge graphs with\n",
    "    entity attributes and multimedia descriptions.\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, rel_model, loss_fn, num_relations, regularizer,\n",
    "                 encoder_name=None, embeddings=None):\n",
    "        super().__init__(rel_model, loss_fn, num_relations, regularizer,\n",
    "                         dim, encoder_name, embeddings)\n",
    "\n",
    "        emb_dim = self.embeddings.embedding_length\n",
    "        self.conv1 = nn.Conv1d(emb_dim, self.dim, kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(self.dim, self.dim, kernel_size=2)\n",
    "\n",
    "    def _encode_entity(self, text):\n",
    "        # Extract word embeddings and mask padding\n",
    "        \n",
    "        all_emb = []\n",
    "        for ent_text in text:\n",
    "            emb = []\n",
    "            #print(ent_text)\n",
    "            for ent in ent_text:\n",
    "                #print(ent)\n",
    "                self.embeddings.embed(ent)\n",
    "                toks = []\n",
    "                for token in ent:\n",
    "                    toks.append(token.embedding)\n",
    "                emb.append(toks)\n",
    "                #print(emb)\n",
    "            all_emb.append(emb)\n",
    "        print(all_emb)\n",
    "        embs = torch.tensor(all_emb)\n",
    "\n",
    "        # Reshape to (N, C, L)\n",
    "        embs = embs.transpose(1, 2)\n",
    "        text_mask = text_mask.unsqueeze(1)\n",
    "\n",
    "        # Pass through CNN, adding padding for valid convolutions\n",
    "        # and masking outputs due to padding\n",
    "        embs = F.pad(embs, [0, 1])\n",
    "        embs = self.conv1(embs)\n",
    "        embs = embs * text_mask\n",
    "        if embs.shape[2] >= 4:\n",
    "            kernel_size = 4\n",
    "        elif embs.shape[2] == 1:\n",
    "            kernel_size = 1\n",
    "        else:\n",
    "            kernel_size = 2\n",
    "        embs = F.max_pool1d(embs, kernel_size=kernel_size)\n",
    "        text_mask = F.max_pool1d(text_mask, kernel_size=kernel_size)\n",
    "        embs = torch.tanh(embs)\n",
    "        embs = F.pad(embs, [0, 1])\n",
    "        embs = self.conv2(embs)\n",
    "        lengths = torch.sum(text_mask, dim=-1)\n",
    "        embs = torch.sum(embs * text_mask, dim=-1) / lengths\n",
    "        embs = torch.tanh(embs)\n",
    "\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-sugar",
   "metadata": {},
   "source": [
    "data file functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "functioning-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import string\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "purple-notification",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/suparnaghanvatkar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/suparnaghanvatkar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "UNK = '[UNK]'\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "DROPPED = STOP_WORDS + list(string.punctuation)\n",
    "CATEGORY_IDS = {'1-to-1': 0, '1-to-many': 1, 'many-to-1': 2, 'many-to-many': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "completed-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_ids(file_path):\n",
    "    \"\"\"Read one line per file and assign it an ID.\n",
    "\n",
    "    Args:\n",
    "        file_path: str, path of file to read\n",
    "\n",
    "    Returns: dict, mapping str to ID (int)\n",
    "    \"\"\"\n",
    "    str2id = dict()\n",
    "    with open(file_path) as file:\n",
    "        for i, line in enumerate(file):\n",
    "            str2id[line.strip()] = i\n",
    "\n",
    "    return str2id\n",
    "\n",
    "\n",
    "def get_negative_sampling_indices(batch_size, num_negatives, repeats=1):\n",
    "    \"\"\"\"Obtain indices for negative sampling within a batch of entity pairs.\n",
    "    Indices are sampled from a reshaped array of indices. For example,\n",
    "    if there are 4 pairs (batch_size=4), the array of indices is\n",
    "        [[0, 1],\n",
    "         [2, 3],\n",
    "         [4, 5],\n",
    "         [6, 7]]\n",
    "    From this array, we corrupt either the first or second element of each row.\n",
    "    This yields one negative sample.\n",
    "    For example, if the positions with a dash are selected,\n",
    "        [[0, -],\n",
    "         [-, 3],\n",
    "         [4, -],\n",
    "         [-, 7]]\n",
    "    they are then replaced with a random index from a row other than the row\n",
    "    to which they belong:\n",
    "        [[0, 3],\n",
    "         [5, 3],\n",
    "         [4, 6],\n",
    "         [1, 7]]\n",
    "    The returned array has shape (batch_size, num_negatives, 2).\n",
    "    \"\"\"\n",
    "    num_ents = batch_size * 2\n",
    "    idx = torch.arange(num_ents).reshape(batch_size, 2)\n",
    "\n",
    "    # For each row, sample entities, assigning 0 probability to entities\n",
    "    # of the same row\n",
    "    zeros = torch.zeros(batch_size, 2)\n",
    "    head_weights = torch.ones(batch_size, num_ents, dtype=torch.float)\n",
    "    head_weights.scatter_(1, idx, zeros)\n",
    "    random_idx = head_weights.multinomial(num_negatives * repeats,\n",
    "                                          replacement=True)\n",
    "    random_idx = random_idx.t().flatten()\n",
    "\n",
    "    # Select randomly the first or the second column\n",
    "    row_selector = torch.arange(batch_size * num_negatives * repeats)\n",
    "    col_selector = torch.randint(0, 2, [batch_size * num_negatives * repeats])\n",
    "\n",
    "    # Fill the array of negative samples with the sampled random entities\n",
    "    # at the right positions\n",
    "    neg_idx = idx.repeat((num_negatives * repeats, 1))\n",
    "    neg_idx[row_selector, col_selector] = random_idx\n",
    "    neg_idx = neg_idx.reshape(-1, batch_size * repeats, 2)\n",
    "    neg_idx.transpose_(0, 1)\n",
    "\n",
    "    return neg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "smaller-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    \"\"\"A Dataset storing the triples of a Knowledge Graph.\n",
    "\n",
    "    Args:\n",
    "        triples_file: str, path to the file containing triples. This is a\n",
    "            text file where each line contains a triple of the form\n",
    "            'subject predicate object'\n",
    "        write_maps_file: bool, if set to True, dictionaries mapping\n",
    "            entities and relations to IDs are saved to disk (for reuse with\n",
    "            other datasets).\n",
    "    \"\"\"\n",
    "    def __init__(self, triples_file, neg_samples=None, write_maps_file=False,\n",
    "                 num_devices=1):\n",
    "        directory = osp.dirname(triples_file)\n",
    "        maps_path = osp.join(directory, 'maps.pt')\n",
    "\n",
    "        # Create or load maps from entity and relation strings to unique IDs\n",
    "        if not write_maps_file:\n",
    "            if not osp.exists(maps_path):\n",
    "                raise ValueError('Maps file not found.')\n",
    "\n",
    "            maps = torch.load(maps_path)\n",
    "            ent_ids, rel_ids = maps['ent_ids'], maps['rel_ids']\n",
    "        else:\n",
    "            ents_file = osp.join(directory, 'entities.txt')\n",
    "            rels_file = osp.join(directory, 'relations.txt')\n",
    "            ent_ids = file_to_ids(ents_file)\n",
    "            rel_ids = file_to_ids(rels_file)\n",
    "\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "\n",
    "        # Read triples and store as ints in tensor\n",
    "        file = open(triples_file)\n",
    "        triples = []\n",
    "        for i, line in enumerate(file):\n",
    "            values = line.split()\n",
    "            # FB13 and WN11 have duplicate triples for classification,\n",
    "            # here we keep the correct triple\n",
    "            if len(values) > 3 and values[3] == '-1':\n",
    "                continue\n",
    "            head, rel, tail = line.split()[:3]\n",
    "            entities.update([head, tail])\n",
    "            relations.add(rel)\n",
    "            triples.append([ent_ids[head], ent_ids[tail], rel_ids[rel]])\n",
    "\n",
    "        self.triples = torch.tensor(triples, dtype=torch.long)\n",
    "\n",
    "        self.rel_categories = torch.zeros(len(rel_ids), dtype=torch.long)\n",
    "        rel_categories_file = osp.join(directory, 'relations-cat.txt')\n",
    "        self.has_rel_categories = False\n",
    "        if osp.exists(rel_categories_file):\n",
    "            with open(rel_categories_file) as f:\n",
    "                for line in f:\n",
    "                    rel, cat = line.strip().split()\n",
    "                    self.rel_categories[rel_ids[rel]] = CATEGORY_IDS[cat]\n",
    "            self.has_rel_categories = True\n",
    "\n",
    "        # Save maps for reuse\n",
    "        torch.save({'ent_ids': ent_ids, 'rel_ids': rel_ids}, maps_path)\n",
    "\n",
    "        self.num_ents = len(entities)\n",
    "        self.num_rels = len(relations)\n",
    "        self.entities = torch.tensor([ent_ids[ent] for ent in entities])\n",
    "        self.num_triples = self.triples.shape[0]\n",
    "        self.directory = directory\n",
    "        self.maps_path = maps_path\n",
    "        self.neg_samples = neg_samples\n",
    "        self.num_devices = num_devices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.triples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_triples\n",
    "\n",
    "    def collate_fn(self, data_list):\n",
    "        \"\"\"Given a batch of triples, return it together with a batch of\n",
    "        corrupted triples where either the subject or object are replaced\n",
    "        by a random entity. Use as a collate_fn for a DataLoader.\n",
    "        \"\"\"\n",
    "        batch_size = len(data_list)\n",
    "        pos_pairs, rels = torch.stack(data_list).split(2, dim=1)\n",
    "        neg_idx = get_negative_sampling_indices(batch_size, self.neg_samples)\n",
    "        return pos_pairs, rels, neg_idx\n",
    "\n",
    "\n",
    "class TextGraphDataset(GraphDataset):\n",
    "    \"\"\"A dataset storing a graph, and textual descriptions of its entities.\n",
    "\n",
    "    Args:\n",
    "        triples_file: str, path to the file containing triples. This is a\n",
    "            text file where each line contains a triple of the form\n",
    "            'subject predicate object'\n",
    "        max_len: int, maximum number of tokens to read per description.\n",
    "        neg_samples: int, number of negative samples to get per triple\n",
    "        tokenizer: transformers.PreTrainedTokenizer or GloVeTokenizer, used\n",
    "            to tokenize the text.\n",
    "        drop_stopwords: bool, if set to True, punctuation and stopwords are\n",
    "            dropped from entity descriptions.\n",
    "        write_maps_file: bool, if set to True, dictionaries mapping\n",
    "            entities and relations to IDs are saved to disk (for reuse with\n",
    "            other datasets).\n",
    "        drop_stopwords: bool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, triples_file, neg_samples,\n",
    "                 drop_stopwords, write_maps_file=False,\n",
    "                 num_devices=1):\n",
    "        super().__init__(triples_file, neg_samples, write_maps_file,\n",
    "                         num_devices)\n",
    "\n",
    "        maps = torch.load(self.maps_path)\n",
    "        ent_ids = maps['ent_ids']\n",
    "\n",
    "\n",
    "        #self.text_data = torch.zeros((len(ent_ids), max_len + 1),\n",
    "        #                             dtype=torch.long)\n",
    "        self.text_data = dict()\n",
    "        read_entities = set()\n",
    "        progress = tqdm(desc='Reading entity descriptions',\n",
    "                        total=len(ent_ids), mininterval=5)\n",
    "        for text_file in ('entity2textlong.txt', 'entity2text.txt'):\n",
    "            file_path = osp.join(self.directory, text_file)\n",
    "            if not osp.exists(file_path):\n",
    "                continue\n",
    "\n",
    "            with open(file_path) as f:\n",
    "                for line in f:\n",
    "                    values = line.strip().split('\\t')\n",
    "                    entity = values[0]\n",
    "                    text = ' '.join(values[1:])\n",
    "                    if entity not in ent_ids:\n",
    "                        continue\n",
    "                    if entity in read_entities:\n",
    "                        continue\n",
    "\n",
    "                    read_entities.add(entity)\n",
    "                    ent_id = ent_ids[entity]\n",
    "\n",
    "                    if drop_stopwords:\n",
    "                        tokens = nltk.word_tokenize(text)\n",
    "                        text = ' '.join([t for t in tokens if\n",
    "                                         t.lower() not in DROPPED])\n",
    "\n",
    "                    #text_tokens = tokenizer.encode(text,\n",
    "                    #                               max_length=max_len,\n",
    "                    #                               return_tensors='pt')\n",
    "                    text_sent = Sentence(text)\n",
    "                    #text_len = text_tokens.shape[1]\n",
    "\n",
    "                    # Starting slice of row contains token IDs\n",
    "                    #print(ent_id)\n",
    "                    self.text_data[ent_id] = text_sent\n",
    "\n",
    "                    progress.update()\n",
    "\n",
    "        progress.close()\n",
    "\n",
    "        if len(read_entities) != len(ent_ids):\n",
    "            raise ValueError(f'Read {len(read_entities):,} descriptions,'\n",
    "                             f' but {len(ent_ids):,} were expected.')\n",
    "\n",
    "        #torch.save(self.text_data,\n",
    "        #           osp.join(self.directory, 'text_data.pt'))\n",
    "\n",
    "    def get_entity_description(self, ent_ids):\n",
    "        \"\"\"Get entity descriptions for a tensor of entity IDs.\"\"\"\n",
    "        all_text = []\n",
    "        for ent_id in ent_ids:\n",
    "            text = []\n",
    "            for ent in ent_id:\n",
    "                text.append(self.text_data[ent.item()])\n",
    "            all_text.append(text)\n",
    "        #print(text)\n",
    "        return all_text\n",
    "    \n",
    "    def collate_fn(self, data_list):\n",
    "        \"\"\"Given a batch of triples, return it in the form of\n",
    "        entity descriptions, and the relation types between them.\n",
    "        Use as a collate_fn for a DataLoader.\n",
    "        \"\"\"\n",
    "        batch_size = len(data_list) // self.num_devices\n",
    "        if batch_size <= 1:\n",
    "            raise ValueError('collate_text can only work with batch sizes'\n",
    "                             ' larger than 1.')\n",
    "        \n",
    "        #print(data_list)\n",
    "        pos_pairs, rels = torch.stack(data_list).split(2, dim=1)\n",
    "        #print(pos_pairs)\n",
    "        text = self.get_entity_description(pos_pairs)\n",
    "\n",
    "        neg_idx = get_negative_sampling_indices(batch_size, self.neg_samples,\n",
    "                                                repeats=self.num_devices)\n",
    "\n",
    "        return text, rels, neg_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-bulletin",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chubby-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "protective-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, dim, rel_model, loss_fn, num_entities, num_relations,\n",
    "              encoder_name, regularizer):\n",
    "    if model == 'bert-dkrl':\n",
    "        return DKRL(dim, rel_model, loss_fn, num_relations, regularizer,\n",
    "                           encoder_name=encoder_name)\n",
    "    elif model == 'glove-dkrl':\n",
    "        return DKRL(dim, rel_model, loss_fn, num_relations, regularizer,\n",
    "                           embeddings='data/glove/glove.6B.300d.pt')\n",
    "    else:\n",
    "        raise ValueError(f'Unkown model {model}')\n",
    "\n",
    "\n",
    "def make_ent2idx(entities, max_ent_id):\n",
    "    \"\"\"Given a tensor with entity IDs, return a tensor indexed with\n",
    "    an entity ID, containing the position of the entity.\n",
    "    Empty positions are filled with -1.\n",
    "\n",
    "    Example:\n",
    "    > make_ent2idx(torch.tensor([4, 5, 0]))\n",
    "    tensor([ 2, -1, -1, -1,  0,  1])\n",
    "    \"\"\"\n",
    "    idx = torch.arange(entities.shape[0])\n",
    "    ent2idx = torch.empty(max_ent_id + 1, dtype=torch.long).fill_(-1)\n",
    "    ent2idx.scatter_(0, entities, idx)\n",
    "    return ent2idx\n",
    "\n",
    "\n",
    "def get_triple_filters(triples, graph, num_ents, ent2idx):\n",
    "    \"\"\"Given a set of triples, filter candidate entities that are valid\n",
    "    substitutes of an entity in the triple at a given position (head or tail).\n",
    "    For a particular triple, this allows to compute rankings for an entity of\n",
    "    interest, against other entities in the graph that would actually be wrong\n",
    "    substitutes.\n",
    "    Results are returned as a mask array with a value of 1.0 for filtered\n",
    "    entities, and 0.0 otherwise.\n",
    "\n",
    "    Args:\n",
    "        triples: Bx3 tensor of type torch.long, where B is the batch size,\n",
    "            and each row contains a triple of the form (head, tail, rel)\n",
    "        graph: nx.MultiDiGraph containing all edges used to filter candidates\n",
    "        num_ents: int, number of candidate entities\n",
    "        ent2idx: tensor, contains at index ent_id the index of the column for\n",
    "            that entity in the output mask array\n",
    "    \"\"\"\n",
    "    num_triples = triples.shape[0]\n",
    "    heads_filter = torch.zeros((num_triples, num_ents), dtype=torch.bool)\n",
    "    tails_filter = torch.zeros_like(heads_filter)\n",
    "\n",
    "    triples = triples.tolist()\n",
    "    for i, (head, tail, rel) in enumerate(triples):\n",
    "        head_edges = graph.out_edges(head, data='weight')\n",
    "        for (h, t, r) in head_edges:\n",
    "            if r == rel and t != tail:\n",
    "                ent_idx = ent2idx[t]\n",
    "                if ent_idx != -1:\n",
    "                    tails_filter[i, ent_idx] = True\n",
    "\n",
    "        tail_edges = graph.in_edges(tail, data='weight')\n",
    "        for (h, t, r) in tail_edges:\n",
    "            if r == rel and h != head:\n",
    "                ent_idx = ent2idx[h]\n",
    "                if ent_idx != -1:\n",
    "                    heads_filter[i, ent_idx] = True\n",
    "\n",
    "    return heads_filter, tails_filter\n",
    "\n",
    "\n",
    "def hit_at_k(predictions, ground_truth_idx, hit_positions):\n",
    "    \"\"\"Calculates mean number of hits@k. Higher values are ranked first.\n",
    "\n",
    "    Args:\n",
    "        predictions: BxN tensor of prediction values where B is batch size\n",
    "            and N number of classes.\n",
    "        ground_truth_idx: Bx1 tensor with index of ground truth class\n",
    "        hit_positions: list, containing number of top K results to be\n",
    "            considered as hits.\n",
    "\n",
    "    Returns: list of float, of the same length as hit_positions, containing\n",
    "        Hits@K score.\n",
    "    \"\"\"\n",
    "    max_position = max(hit_positions)\n",
    "    _, indices = predictions.topk(k=max_position)\n",
    "    hits = []\n",
    "\n",
    "    for position in hit_positions:\n",
    "        idx_at_k = indices[:, :position]\n",
    "        hits_at_k = (idx_at_k == ground_truth_idx).sum(dim=1).float().mean()\n",
    "        hits.append(hits_at_k.item())\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "def mrr(predictions, ground_truth_idx):\n",
    "    \"\"\"Calculates mean reciprocal rank (MRR) for given predictions\n",
    "    and ground truth values. Higher values are ranked first.\n",
    "\n",
    "    Args:\n",
    "        predictions: BxN tensor of prediction values where B is batch size\n",
    "            and N number of classes.\n",
    "        ground_truth_idx: Bx1 tensor with index of ground truth class\n",
    "\n",
    "    Returns: float, Mean reciprocal rank score\n",
    "    \"\"\"\n",
    "    indices = predictions.argsort(descending=True)\n",
    "    rankings = (indices == ground_truth_idx).nonzero()[:, 1].float() + 1.0\n",
    "    return rankings.reciprocal()\n",
    "\n",
    "\n",
    "def split_by_new_position(triples, mrr_values, new_entities):\n",
    "    \"\"\"Split MRR results by the position of new entity. Use to break down\n",
    "    results for a triple where a new entity is at the head and the tail,\n",
    "    at the head only, or the tail only.\n",
    "    Since MRR is calculated by corrupting the head first, and then the head,\n",
    "    the size of mrr_values should be twice the size of triples. The calculated\n",
    "    MRR is then the average of the two cases.\n",
    "    Args:\n",
    "        triples: Bx3 tensor containing (head, tail, rel).\n",
    "        mrr_values: 2B tensor, with first half containing MRR for corrupted\n",
    "            triples at the head position, and second half at the tail position.\n",
    "        new_entities: set, entities to be considered as new.\n",
    "    Returns:\n",
    "        mrr_by_position: tensor of 3 elements breaking down MRR by new entities\n",
    "            at both positions, at head, and tail.\n",
    "        mrr_pos_counts: tensor of 3 elements containing counts for each case.\n",
    "    \"\"\"\n",
    "    mrr_by_position = torch.zeros(3, device=mrr_values.device)\n",
    "    mrr_pos_counts = torch.zeros_like(mrr_by_position)\n",
    "    num_triples = triples.shape[0]\n",
    "\n",
    "    for i, (h, t, r) in enumerate(triples):\n",
    "        head, tail = h.item(), t.item()\n",
    "        mrr_val = (mrr_values[i] + mrr_values[i + num_triples]).item() / 2.0\n",
    "        if head in new_entities and tail in new_entities:\n",
    "            mrr_by_position[0] += mrr_val\n",
    "            mrr_pos_counts[0] += 1.0\n",
    "        elif head in new_entities:\n",
    "            mrr_by_position[1] += mrr_val\n",
    "            mrr_pos_counts[1] += 1.0\n",
    "        elif tail in new_entities:\n",
    "            mrr_by_position[2] += mrr_val\n",
    "            mrr_pos_counts[2] += 1.0\n",
    "\n",
    "    return mrr_by_position, mrr_pos_counts\n",
    "\n",
    "\n",
    "def split_by_category(triples, mrr_values, rel_categories):\n",
    "    mrr_by_category = torch.zeros([2, 4], device=mrr_values.device)\n",
    "    mrr_cat_count = torch.zeros([1, 4], dtype=torch.float,\n",
    "                                device=mrr_by_category.device)\n",
    "    num_triples = triples.shape[0]\n",
    "\n",
    "    for i, (h, t, r) in enumerate(triples):\n",
    "        rel_category = rel_categories[r]\n",
    "\n",
    "        mrr_val_head_pred = mrr_values[i]\n",
    "        mrr_by_category[0, rel_category] += mrr_val_head_pred\n",
    "\n",
    "        mrr_val_tail_pred = mrr_values[i + num_triples]\n",
    "        mrr_by_category[1, rel_category] += mrr_val_tail_pred\n",
    "\n",
    "        mrr_cat_count[0, rel_category] += 1\n",
    "\n",
    "    return mrr_by_category, mrr_cat_count\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    \"\"\"Get a default logger that includes a timestamp.\"\"\"\n",
    "    logger = logging.getLogger(\"\")\n",
    "    logger.handlers = []\n",
    "    ch = logging.StreamHandler()\n",
    "    str_fmt = '%(asctime)s - %(levelname)s - %(name)s - %(message)s'\n",
    "    formatter = logging.Formatter(str_fmt, datefmt='%H:%M:%S')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel('INFO')\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arranged-washer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUT_PATH = 'output/'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "urban-execution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vocal-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_link_prediction(model, triples_loader, text_dataset, entities,\n",
    "                         epoch, emb_batch_size,\n",
    "                         prefix='', max_num_batches=None,\n",
    "                         filtering_graph=None, new_entities=None,\n",
    "                         return_embeddings=False):\n",
    "    compute_filtered = filtering_graph is not None\n",
    "    mrr_by_position = torch.zeros(3, dtype=torch.float).to(device)\n",
    "    mrr_pos_counts = torch.zeros_like(mrr_by_position)\n",
    "\n",
    "    rel_categories = triples_loader.dataset.rel_categories.to(device)\n",
    "    mrr_by_category = torch.zeros([2, 4], dtype=torch.float).to(device)\n",
    "    mrr_cat_count = torch.zeros([1, 4], dtype=torch.float).to(device)\n",
    "\n",
    "    hit_positions = [1, 3, 10]\n",
    "    hits_at_k = {pos: 0.0 for pos in hit_positions}\n",
    "    mrr_value = 0.0\n",
    "    mrr_filt = 0.0\n",
    "    hits_at_k_filt = {pos: 0.0 for pos in hit_positions}\n",
    "\n",
    "    if device != torch.device('cpu'):\n",
    "        model = model.module\n",
    "\n",
    "    if isinstance(model, InductiveLinkPrediction):\n",
    "        num_entities = entities.shape[0]\n",
    "        if compute_filtered:\n",
    "            max_ent_id = max(filtering_graph.nodes)\n",
    "        else:\n",
    "            max_ent_id = entities.max()\n",
    "        ent2idx = make_ent2idx(entities, max_ent_id)\n",
    "    else:\n",
    "        print(\"Error in model type not InductiveLink Pred\")\n",
    "        return -1\n",
    "\n",
    "    # Create embedding lookup table for evaluation\n",
    "    ent_emb = torch.zeros((num_entities, model.dim), dtype=torch.float,\n",
    "                          device=device)\n",
    "    idx = 0\n",
    "    num_iters = np.ceil(num_entities / emb_batch_size)\n",
    "    iters_count = 0\n",
    "    while idx < num_entities:\n",
    "        # Get a batch of entity IDs and encode them\n",
    "        batch_ents = entities[idx:idx + emb_batch_size]\n",
    "\n",
    "        if isinstance(model, InductiveLinkPrediction):\n",
    "            # Encode with entity descriptions\n",
    "            data = text_dataset.get_entity_description(batch_ents)\n",
    "            batch_emb = model(data.to(device))\n",
    "        else:\n",
    "            # Encode from lookup table\n",
    "            batch_emb = model(batch_ents)\n",
    "\n",
    "        ent_emb[idx:idx + batch_ents.shape[0]] = batch_emb\n",
    "\n",
    "        iters_count += 1\n",
    "        if iters_count % np.ceil(0.2 * num_iters) == 0:\n",
    "            print(f'[{idx + batch_ents.shape[0]:,}/{num_entities:,}]')\n",
    "\n",
    "        idx += emb_batch_size\n",
    "\n",
    "    ent_emb = ent_emb.unsqueeze(0)\n",
    "\n",
    "    batch_count = 0\n",
    "    print('Computing metrics on set of triples')\n",
    "    total = len(triples_loader) if max_num_batches is None else max_num_batches\n",
    "    for i, triples in enumerate(triples_loader):\n",
    "        if max_num_batches is not None and i == max_num_batches:\n",
    "            break\n",
    "\n",
    "        heads, tails, rels = torch.chunk(triples, chunks=3, dim=1)\n",
    "        # Map entity IDs to positions in ent_emb\n",
    "        heads = ent2idx[heads].to(device)\n",
    "        tails = ent2idx[tails].to(device)\n",
    "\n",
    "        assert heads.min() >= 0\n",
    "        assert tails.min() >= 0\n",
    "\n",
    "        # Embed triple\n",
    "        head_embs = ent_emb.squeeze()[heads]\n",
    "        tail_embs = ent_emb.squeeze()[tails]\n",
    "        rel_embs = model.rel_emb(rels.to(device))\n",
    "\n",
    "        # Score all possible heads and tails\n",
    "        heads_predictions = model.score_fn(ent_emb, tail_embs, rel_embs)\n",
    "        tails_predictions = model.score_fn(head_embs, ent_emb, rel_embs)\n",
    "\n",
    "        pred_ents = torch.cat((heads_predictions, tails_predictions))\n",
    "        true_ents = torch.cat((heads, tails))\n",
    "\n",
    "        hits = hit_at_k(pred_ents, true_ents, hit_positions)\n",
    "        for j, h in enumerate(hits):\n",
    "            hits_at_k[hit_positions[j]] += h\n",
    "        mrr_value += mrr(pred_ents, true_ents).mean().item()\n",
    "\n",
    "        if compute_filtered:\n",
    "            filters = get_triple_filters(triples, filtering_graph,\n",
    "                                               num_entities, ent2idx)\n",
    "            heads_filter, tails_filter = filters\n",
    "            # Filter entities by assigning them the lowest score in the batch\n",
    "            filter_mask = torch.cat((heads_filter, tails_filter)).to(device)\n",
    "            pred_ents[filter_mask] = pred_ents.min() - 1.0\n",
    "            hits_filt = hit_at_k(pred_ents, true_ents, hit_positions)\n",
    "            for j, h in enumerate(hits_filt):\n",
    "                hits_at_k_filt[hit_positions[j]] += h\n",
    "            mrr_filt_per_triple = mrr(pred_ents, true_ents)\n",
    "            mrr_filt += mrr_filt_per_triple.mean().item()\n",
    "\n",
    "            if new_entities is not None:\n",
    "                by_position = split_by_new_position(triples,\n",
    "                                                          mrr_filt_per_triple,\n",
    "                                                          new_entities)\n",
    "                batch_mrr_by_position, batch_mrr_pos_counts = by_position\n",
    "                mrr_by_position += batch_mrr_by_position\n",
    "                mrr_pos_counts += batch_mrr_pos_counts\n",
    "\n",
    "            if triples_loader.dataset.has_rel_categories:\n",
    "                by_category = split_by_category(triples,\n",
    "                                                      mrr_filt_per_triple,\n",
    "                                                      rel_categories)\n",
    "                batch_mrr_by_cat, batch_mrr_cat_count = by_category\n",
    "                mrr_by_category += batch_mrr_by_cat\n",
    "                mrr_cat_count += batch_mrr_cat_count\n",
    "\n",
    "        batch_count += 1\n",
    "        if (i + 1) % int(0.2 * total) == 0:\n",
    "            print(f'[{i + 1:,}/{total:,}]')\n",
    "\n",
    "    for hits_dict in (hits_at_k, hits_at_k_filt):\n",
    "        for k in hits_dict:\n",
    "            hits_dict[k] /= batch_count\n",
    "\n",
    "    mrr_value = mrr_value / batch_count\n",
    "    mrr_filt = mrr_filt / batch_count\n",
    "\n",
    "    log_str = f'{prefix} mrr: {mrr_value:.4f}  '\n",
    "    print(f'{prefix}_mrr', mrr_value, epoch)\n",
    "    for k, value in hits_at_k.items():\n",
    "        log_str += f'hits@{k}: {value:.4f}  '\n",
    "        print(f'{prefix}_hits@{k}', value, epoch)\n",
    "\n",
    "    if compute_filtered:\n",
    "        log_str += f'mrr_filt: {mrr_filt:.4f}  '\n",
    "        print(f'{prefix}_mrr_filt', mrr_filt, epoch)\n",
    "        for k, value in hits_at_k_filt.items():\n",
    "            log_str += f'hits@{k}_filt: {value:.4f}  '\n",
    "            print(f'{prefix}_hits@{k}_filt', value, epoch)\n",
    "\n",
    "    print(log_str)\n",
    "\n",
    "    if new_entities is not None and compute_filtered:\n",
    "        mrr_pos_counts[mrr_pos_counts < 1.0] = 1.0\n",
    "        mrr_by_position = mrr_by_position / mrr_pos_counts\n",
    "        log_str = ''\n",
    "        for i, t in enumerate((f'{prefix}_mrr_filt_both_new',\n",
    "                               f'{prefix}_mrr_filt_head_new',\n",
    "                               f'{prefix}_mrr_filt_tail_new')):\n",
    "            value = mrr_by_position[i].item()\n",
    "            log_str += f'{t}: {value:.4f}  '\n",
    "            print(t, value, epoch)\n",
    "        print(log_str)\n",
    "\n",
    "    if compute_filtered and triples_loader.dataset.has_rel_categories:\n",
    "        mrr_cat_count[mrr_cat_count < 1.0] = 1.0\n",
    "        mrr_by_category = mrr_by_category / mrr_cat_count\n",
    "\n",
    "        for i, case in enumerate(['pred_head', 'pred_tail']):\n",
    "            log_str = f'{case} '\n",
    "            for cat, cat_id in CATEGORY_IDS.items():\n",
    "                log_str += f'{cat}_mrr: {mrr_by_category[i, cat_id]:.4f}  '\n",
    "            print(log_str)\n",
    "\n",
    "    if return_embeddings:\n",
    "        out = (mrr_value, ent_emb)\n",
    "    else:\n",
    "        out = (mrr_value, None)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intermediate-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_prediction(dataset, inductive, dim, model, rel_model, loss_fn,\n",
    "                    encoder_name, regularizer, max_len, num_negatives, lr,\n",
    "                    use_scheduler, batch_size, emb_batch_size, eval_batch_size,\n",
    "                    max_epochs, checkpoint):\n",
    "    drop_stopwords = model in {'bert-dkrl', 'glove-dkrl'}\n",
    "\n",
    "    prefix = 'ind-' if inductive and model != 'transductive' else ''\n",
    "    triples_file = f'data/{dataset}/{prefix}train.tsv'\n",
    "\n",
    "    if device != torch.device('cpu'):\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        if batch_size % num_devices != 0:\n",
    "            raise ValueError(f'Batch size ({batch_size}) must be a multiple of'\n",
    "                             f' the number of CUDA devices ({num_devices})')\n",
    "        print(f'CUDA devices used: {num_devices}')\n",
    "    else:\n",
    "        num_devices = 1\n",
    "        print('Training on CPU')\n",
    "\n",
    "    if model == 'transductive':\n",
    "        train_data = GraphDataset(triples_file, num_negatives,\n",
    "                                  write_maps_file=True,\n",
    "                                  num_devices=num_devices)\n",
    "    else:\n",
    "        train_data = TextGraphDataset(triples_file, num_negatives,\n",
    "                                      drop_stopwords,\n",
    "                                      write_maps_file=True,\n",
    "                                      num_devices=num_devices)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True,\n",
    "                              collate_fn=train_data.collate_fn,\n",
    "                              num_workers=0, drop_last=True)\n",
    "\n",
    "    train_eval_loader = DataLoader(train_data, eval_batch_size)\n",
    "\n",
    "    valid_data = GraphDataset(f'data/{dataset}/{prefix}dev.tsv')\n",
    "    valid_loader = DataLoader(valid_data, eval_batch_size)\n",
    "\n",
    "    test_data = GraphDataset(f'data/{dataset}/{prefix}test.tsv')\n",
    "    test_loader = DataLoader(test_data, eval_batch_size)\n",
    "\n",
    "    # Build graph with all triples to compute filtered metrics\n",
    "    if dataset != 'Wikidata5M':\n",
    "        graph = nx.MultiDiGraph()\n",
    "        all_triples = torch.cat((train_data.triples,\n",
    "                                 valid_data.triples,\n",
    "                                 test_data.triples))\n",
    "        graph.add_weighted_edges_from(all_triples.tolist())\n",
    "\n",
    "        train_ent = set(train_data.entities.tolist())\n",
    "        train_val_ent = set(valid_data.entities.tolist()).union(train_ent)\n",
    "        train_val_test_ent = set(test_data.entities.tolist()).union(train_val_ent)\n",
    "        val_new_ents = train_val_ent.difference(train_ent)\n",
    "        test_new_ents = train_val_test_ent.difference(train_val_ent)\n",
    "    else:\n",
    "        graph = None\n",
    "\n",
    "        train_ent = set(train_data.entities.tolist())\n",
    "        train_val_ent = set(valid_data.entities.tolist())\n",
    "        train_val_test_ent = set(test_data.entities.tolist())\n",
    "        val_new_ents = test_new_ents = None\n",
    "\n",
    "    print('num_train_entities', len(train_ent))\n",
    "\n",
    "    train_ent = torch.tensor(list(train_ent))\n",
    "    train_val_ent = torch.tensor(list(train_val_ent))\n",
    "    train_val_test_ent = torch.tensor(list(train_val_test_ent))\n",
    "\n",
    "    model = get_model(model, dim, rel_model, loss_fn,\n",
    "                            len(train_val_test_ent), train_data.num_rels,\n",
    "                            encoder_name, regularizer)\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location='cpu'))\n",
    "\n",
    "    if device != torch.device('cpu'):\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * max_epochs\n",
    "    if use_scheduler:\n",
    "        warmup = int(0.2 * total_steps)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=warmup,\n",
    "                                                    num_training_steps=total_steps)\n",
    "    best_valid_mrr = 0.0\n",
    "    checkpoint_file = osp.join(OUT_PATH, f'model-base.pt')\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss = 0\n",
    "        for step, data in enumerate(train_loader):\n",
    "            loss = model(*data).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if use_scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if step % int(0.05 * len(train_loader)) == 0:\n",
    "                print(f'Epoch {epoch}/{max_epochs} '\n",
    "                          f'[{step}/{len(train_loader)}]: {loss.item():.6f}')\n",
    "                print('batch_loss', loss.item())\n",
    "\n",
    "        print('train_loss', train_loss / len(train_loader), epoch)\n",
    "\n",
    "        if dataset != 'Wikidata5M':\n",
    "            print('Evaluating on sample of training set')\n",
    "            eval_link_prediction(model, train_eval_loader, train_data, train_ent,\n",
    "                                 epoch, emb_batch_size, prefix='train',\n",
    "                                 max_num_batches=len(valid_loader))\n",
    "\n",
    "        print('Evaluating on validation set')\n",
    "        val_mrr, _ = eval_link_prediction(model, valid_loader, train_data,\n",
    "                                          train_val_ent, epoch,\n",
    "                                          emb_batch_size, prefix='valid')\n",
    "\n",
    "        # Keep checkpoint of best performing model (based on raw MRR)\n",
    "        if val_mrr > best_valid_mrr:\n",
    "            best_valid_mrr = val_mrr\n",
    "            torch.save(model.state_dict(), checkpoint_file)\n",
    "\n",
    "    # Evaluate with best performing checkpoint\n",
    "    if max_epochs > 0:\n",
    "        model.load_state_dict(torch.load(checkpoint_file))\n",
    "\n",
    "    if dataset == 'Wikidata5M':\n",
    "        graph = nx.MultiDiGraph()\n",
    "        graph.add_weighted_edges_from(valid_data.triples.tolist())\n",
    "\n",
    "    print('Evaluating on validation set (with filtering)')\n",
    "    eval_link_prediction(model, valid_loader, train_data, train_val_ent,\n",
    "                         max_epochs + 1, emb_batch_size, prefix='valid',\n",
    "                         filtering_graph=graph,\n",
    "                         new_entities=val_new_ents)\n",
    "\n",
    "    if dataset == 'Wikidata5M':\n",
    "        graph = nx.MultiDiGraph()\n",
    "        graph.add_weighted_edges_from(test_data.triples.tolist())\n",
    "\n",
    "    print('Evaluating on test set')\n",
    "    _, ent_emb = eval_link_prediction(model, test_loader, train_data,\n",
    "                                      train_val_test_ent, max_epochs + 1,\n",
    "                                      emb_batch_size, prefix='test',\n",
    "                                      filtering_graph=graph,\n",
    "                                      new_entities=test_new_ents,\n",
    "                                      return_embeddings=True)\n",
    "\n",
    "    # Save final entity embeddings obtained with trained encoder\n",
    "    #torch.save(ent_emb, osp.join(OUT_PATH, f'ent_emb-base.pt'))\n",
    "    #torch.save(train_val_test_ent, osp.join(OUT_PATH, f'ents-base.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sized-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading entity descriptions: 100%|| 14541/14541 [00:25<00:00, 559.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_entities 11633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-747fc3ed8fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m link_prediction(dataset='FB15k-237', inductive=True, dim=128, model='bert-dkrl', rel_model='transe', loss_fn='margin', \\\n\u001b[1;32m      2\u001b[0m                     \u001b[0mencoder_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_negatives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     max_epochs=1, checkpoint=None)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-276463eda964>\u001b[0m in \u001b[0;36mlink_prediction\u001b[0;34m(dataset, inductive, dim, model, rel_model, loss_fn, encoder_name, regularizer, max_len, num_negatives, lr, use_scheduler, batch_size, emb_batch_size, eval_batch_size, max_epochs, checkpoint)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/blp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-373cd9126f84>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, rels, neg_idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Encode text into an entity representation from its description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0ment_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mneg_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2c3d6baa298a>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0ment_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_embs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0ment_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4f09df2cf3d8>\u001b[0m in \u001b[0;36m_encode_entity\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mall_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Reshape to (N, C, L)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "link_prediction(dataset='FB15k-237', inductive=True, dim=128, model='bert-dkrl', rel_model='transe', loss_fn='margin', \\\n",
    "                    encoder_name='bert-base-cased', regularizer=1e-2, max_len=32, num_negatives=64, lr=1e-4, use_scheduler=False, batch_size=64, emb_batch_size=512, eval_batch_size=128,\\\n",
    "                    max_epochs=1, checkpoint=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
